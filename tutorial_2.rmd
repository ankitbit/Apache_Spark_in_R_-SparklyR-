

### Using the function starts_with( ) and ends_with( )
```{r }
# track_metadata_tbl has been pre-defined
track_metadata_tbl

track_metadata_tbl %>%
  # Select columns starting with artist
  select(starts_with("artist"))
track_metadata_tbl %>%
  # Select columns ending with id
  select(ends_with("id"))
```

```{r}
# track_metadata_tbl has been pre-defined
track_metadata_tbl

track_metadata_tbl %>%
  # Select columns containing ti
  select(contains("ti"))

track_metadata_tbl %>%
  # Select columns matching ti.?t
  select(matches("ti.?t"))
```

**NOTE:** **copy_to( ) moves your data from R to Spark; collect( ) goes in the opposite direction**
```{r}
# track_metadata_tbl has been pre-defined
track_metadata_tbl

results <- track_metadata_tbl %>%
  # Filter where artist familiarity is greater than 0.9
  filter(artist_familiarity>0.9)

# Examine the class of the results
class(results)

# Collect your results
collected <- results %>%collect()

# Examine the class of the collected results
class(collected)
```
#### Storing intermediate results
As you saw in Chapter 1, copying data between R and Spark is a fundamentally slow task. That means that collecting the data, as you saw in the previous exercise, should only be done when you really need to.

The pipe operator is really nice for chaining together data manipulation commands, but in general, you can't do a whole analysis with everything chained together. For example, this is an awful practice, since you will never be able to debug your code.
```{r}
final_results <- starting_data %>%
  # 743 steps piped together
  # ... %>%
  collect()
```
That gives a dilemma. You need to store the results of intermediate calculations, but you don't want to collect them because it is slow. The solution is to use compute() to compute the calculation, but store the results in a temporary data frame on Spark. Compute takes two arguments: a tibble, and a variable name for the Spark data frame that will store the results.
```{r}
a_tibble %>%
  # some calculations %>%
  compute("intermediate_results")
```

A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.

Filter the rows of track_metadata_tbl where artist_familiarity is greater than 0.8.
Compute the results using compute().
Store the results in a Spark data frame named "familiar_artists".
Assign the result to an R tibble named computed.
See the available Spark datasets using src_tbls().
Print the class() of computed. Notice that unlike collect(), compute() returns a remote tibble. The data is still stored in the Spark cluster.

```{r}
# track_metadata_tbl has been pre-defined
track_metadata_tbl

computed <- track_metadata_tbl %>%
  # Filter where artist familiarity is greater than 0.8
  filter(artist_familiarity > 0.8) %>%
  # Compute the results
  compute("familiar_artists")

# See the available datasets
# pass the spark connection name as the argument to src_tbls()
src_tbls(spark_conn) 

# Examine the class of the computed results
class(computed)
```
**NOTE: class(computed) is tbl_lazy. compute() lets you store intermediate results, without having to copy data to R.**

#### Groups: great for music, great for data
A common analysis problem is how to calculate summary statistics for each group of data. 
For example, you might want to know your sales revenues by month, or by region. 
In R, the process of splitting up your data into groups, applying a summary statistic on each group, 
and combining the results into a single data structure, is known as "**split-apply-combine**". 
The concept is much older though: SQL has had the GROUP BY statement for decades. 
The term "**map-reduce**" is a similar concept, where "**map**" is very roughly analogous to the "**split**" 
and "apply" steps, and "reducing" is "combining". The dplyr/sparklyr approach is to use group_by() 
before you mutate() or summarize(). It takes the unquoted names of columns to group by. 
For example, to calculate the mean of column x, for each combination of values in columns grp1 and 
grp2, you would write the following.

```{r}
a_tibble %>%
  group_by(grp1, grp2) %>%
  summarize(mean_x = mean(x))
```
**Note that the columns passed to group_by() should typically be categorical variables.** 

For example, if you wanted to calculate the average weight of people relative to their height, it doesn't make sense to group by height, since everyone's height is unique. You could, however, use **cut()** to convert the heights into different categories, and calculate the mean weight for each category.
```{r}
# track_metadata_tbl has been pre-defined
track_metadata_tbl

duration_by_artist <- track_metadata_tbl %>%
  # Group by artist
  group_by(artist_name) %>%
  # Calc mean duration
  summarise(mean_duration=mean(duration)

duration_by_artist %>%
  # Sort by ascending mean duration
  arrange(mean_duration)

duration_by_artist %>%
  # Sort by descending mean duration
  arrange(desc(mean_duration)
```
**NOTE: summarize( ) works with grouped tibbles, and as you'll see next, so does mutate( )**
